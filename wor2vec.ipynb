{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc398f78-469b-4b36-8dfd-b7a90726ec61",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4692931-3441-483e-b2e5-650d33a3eb6e",
   "metadata": {},
   "source": [
    "### trained gensim.models.Word2Vec with df['Desc'], created a visualization,reduced the dimantion with PCA to 15 and created a new dataframe with the new 15 features and the original dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6381d-93d9-41b4-92fb-2efc79ae762a",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b535b6-6371-46d7-a07b-f55fede84e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hana Baron\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1de7e6da7392>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Desc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "corpus = df['Desc']\n",
    "model = gensim.models.Word2Vec(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31cf2e-d0ed-4986-841a-2e52724572c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = model.wv['software']\n",
    "vec_king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9980ce9-9b67-4d06-ad87-7bd4bc7a5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b8b83f-7624-473a-90ce-7c118ddcfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, word in enumerate(model.wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(\"index is \" , index, \"word is \", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19215467-6502-4d73-bf94-8cec0ced13da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
    "vector_list = [model.wv[word] for word in words if word in model.wv.index_to_key]\n",
    "\n",
    "# Create a list of the words corresponding to these vectors\n",
    "words_filtered = [word for word in words if word in model.wv.index_to_key]\n",
    "\n",
    "# Zip the words together with their vector representations\n",
    "word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "# Cast to a dict so we can turn it into a DataFrame\n",
    "word_vec_dict = dict(word_vec_zip)\n",
    "df_vec = pd.DataFrame.from_dict(word_vec_dict, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e63b2f-4e39-4c96-bf3c-47fac71a816f",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6670a-f52e-4a62-8e62-78b3afae3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a0dcd-b96b-4442-b4eb-ce2d4a512dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "######visualization of word2vec\n",
    "# %matplotlib inline  \n",
    "# %matplotlib ipympl\n",
    "# %matplotlib notebook\n",
    "# %pylab\n",
    "\n",
    "word = \"software\"\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "## word embedding\n",
    "tot_words = [word] + [tupla[0] for tupla in model.wv.most_similar(word, topn=20)]\n",
    "X = model.wv[tot_words]\n",
    "## pca to reduce dimensionality from 300 to 3\n",
    "pca = manifold.TSNE(perplexity=40, n_components=3, init='pca')\n",
    "X = pca.fit_transform(X)\n",
    "## create dtf\n",
    "dtf_ = pd.DataFrame(X, index=tot_words, columns=[\"x\",\"y\",\"z\"])\n",
    "dtf_[\"input\"] = 0\n",
    "dtf_[\"input\"].iloc[0:1] = 1\n",
    "\n",
    "## plot 3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(dtf_[dtf_[\"input\"]==0]['x'], \n",
    "           dtf_[dtf_[\"input\"]==0]['y'], \n",
    "           dtf_[dtf_[\"input\"]==0]['z'], c=\"black\")\n",
    "ax.scatter(dtf_[dtf_[\"input\"]==1]['x'], \n",
    "           dtf_[dtf_[\"input\"]==1]['y'], \n",
    "           dtf_[dtf_[\"input\"]==1]['z'], c=\"red\")\n",
    "ax.set(xlabel=None, ylabel=None, zlabel=None, xticklabels=[], \n",
    "       yticklabels=[], zticklabels=[])\n",
    "for label, row in dtf_[[\"x\",\"y\",\"z\"]].iterrows():\n",
    "    x, y, z = row\n",
    "    ax.text(x, y, z, s=label)\n",
    "# ax.mouse_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978f4c1-3070-46ca-a307-1d72f23cc4d2",
   "metadata": {},
   "source": [
    "#### Reducing dimantions with PCA and creating new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d4986-648e-4f75-a750-4288954b1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in model.wv.index_to_key]\n",
    "    return np.mean(model.wv[doc], axis=0)\n",
    "\n",
    "# Function that will help us drop documents that have no word vectors in word2vec\n",
    "def has_vector_representation(word2vec_model, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in word2vec_model.wv.index_to_key for word in doc)\n",
    "\n",
    "# Filter out documents\n",
    "def filter_docs(corpus, texts, condition_on_doc):\n",
    "    \"\"\"\n",
    "    Filter corpus and texts given the function condition_on_doc which takes a doc. The document doc is kept if condition_on_doc(doc) is true.\n",
    "    \"\"\"\n",
    "    number_of_docs = len(corpus)\n",
    "\n",
    "    if texts is not None:\n",
    "        texts = [text for (text, doc) in zip(texts, corpus)\n",
    "                 if condition_on_doc(doc)]\n",
    "\n",
    "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
    "\n",
    "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
    "\n",
    "    return (corpus, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f4972-613a-4da1-8e98-cfa5b0fb2cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove docs that don't include any words in W2V's vocab\n",
    "corpus, desc_list = filter_docs(corpus, df['Desc'], lambda doc: has_vector_representation(model, doc))\n",
    "\n",
    "# Filter out any empty docs\n",
    "corpus, desc_list = filter_docs(corpus, desc_list, lambda doc: (len(doc) != 0))\n",
    "x = []\n",
    "for doc in corpus: # append the vector for each document\n",
    "    x.append(document_vector(model, doc))\n",
    "    \n",
    "X = np.array(x) # list to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460eb31-1fde-4a66-8e8c-f57969f7a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=15, random_state=10)\n",
    "\n",
    "# as a reminder, x is the array with our 300-dimensional vectors\n",
    "reduced_vecs = pca.fit_transform(x)\n",
    "df_w_vectors = pd.DataFrame(reduced_vecs)\n",
    "\n",
    "# Compose dataframe \n",
    "df_w_vectors['Desc'] = df['Desc']\n",
    "# Use pd.concat to match original titles with their vectors\n",
    "main_w_vectors = pd.concat((df_w_vectors, df), axis=1)\n",
    "\n",
    "# Get rid of vectors that couldn't be matched with the main_df\n",
    "main_w_vectors.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4850e843-03b5-4932-9a29-7caa5b44e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_w_vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
